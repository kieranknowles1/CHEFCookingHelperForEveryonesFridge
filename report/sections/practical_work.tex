\todoinline{
    Practical Work
    This is a description and discussion of the practical work you have carried out. You have defined how it is to be assessed in the Terms of Reference.
    How to organise the practical work chapters varies between projects, depending to the nature of your practical work. If you have conducted an investigative work
    without the aid of a product, you should organise the contents into three chapters as Design, Investigation and Results. However, if your practical work is mainly
    the development of a product, it makes more sense that you organise the chapters in terms of the product development life cycle, i.e., Design, Implementation and Testing.
    Please choose from below the appropriate section to read considering the nature of your own project.

    â€¢	Projects focusing on the development of a product
    You should discuss the design, implementation, and testing of your product. There are sections on each of these below. If there were other activities involved in development,
    such as analysis modelling based on the requirements specification, or if your project involved other practical work such as an experiment using your product, you should also
    include these: say what you did and discuss any interesting decisions or problems. If your product does not fit this model --- for example, a project whose product is an
    information strategy --- this section should discuss the work needed to create that product.

    The narrative should especially identify areas of the work that were particularly interesting or difficult. Assume that the readership of the report will be computer
    literate individuals who will appreciate the problems you have tackled.
    Justify in detail the methods you chose to synthesise a solution to the problem. Discuss how your reading of the literature guided you in your work. You may wish to
    refer to supporting documentation in your discussion of the solution; these will be held in appendices to the report.
    In general, there should be neither bookwork nor theoretical material here. You should tell the reader what you did, why you did it and how you did it. Unless you have
    developed a worthwhile product or solved a challenging problem there will be little for you to say (and few marks to gain).
}

\section{Design}

\begin{figure}
    \centering
    \includesvg{figures/Architecture}
    \caption{\label{fig:architecture}Architecture of the App}
\end{figure}

\subsection{Backend Design}
\todo{Backend design}

The backend of the \chef{} app was the first component to be designed in order to give the frontend a data source to work with.

\subsection{Frontend Design}
\todo{Frontend design}
Design of the \chef{} app began with creating interface prototypes (see figures~\ref{fig:proto_home},~\ref{fig:proto_find_recipes}, and~\ref{fig:proto_account})

\todoinline{
    Good products, whether software or hardware, must be designed. It is not professional to hack out a solution! You must describe and provide a rationale for that design.
    The artefacts produced are models (and perhaps prototypes) that form part of your product. In the report you tell the reader about your design and discuss the design
    decisions. Throughout the design section you should justify your choices. Discuss the implications of making different design choices, and the reasons for the design
    that you have selected.

    The design chapters should give a top-level view of how your product meets its requirements. For a software product, a good starting point is to describe the architecture
    of the software. (If your product is not software, what corresponds to the software architecture?)  For example, suppose you are going to produce a computer game that
    could be played across the web. This will involve some software concerned with the communications across the network, some software concerned with the specific game and
    some with aspects that could be common to many games. This suggests an architecture consisting of three subsystems. You may feel there are other possible designs. If so,
    discuss each and then tell the reader which you decided to follow, and why.

    Once you have selected a top-level design you can start to look at the details of each product component. You will produce design models as required by the development
    approach that you are following, and you will need to discuss your design decisions. For example, if you are using an object-oriented approach, you will probably describe
    and justify the important classes in your system. Design patterns are an area of increasing popularity and usefulness. Investigate making use of some. Explain which you
    have used and why.

    Make careful use of figures and diagrams when describing the design. Any diagrams are there to help the reader understand what you have done. They must form a minor
    part of the chapter. The full design documentation will be marked under the product marking part of the module.
    Another aspect of the design, which you may wish to write about, is the user interface. There is no point in simply relating HCI theory here, and merely describing
    or giving pictures of your screen designs is also inadequate: what the reader wants to know is how you have applied the theory. Justify your design choices in terms of
    usability principles, and illustrate them with a few carefully selected screen dumps.

    You may wish to discuss the design process that you followed. Theoretical descriptions of design processes are unlikely to be interesting here. How you applied the
    process and how it affected your product, might be.
}

\section{Implementation}

The \chef{} app is implemented using a React frontend and an Express.js backend, both written in TypeScript.
This language was chosen as it compiles into JavaScript that can run in a browser, allowing the same language to be used for both
the front and back ends, while adding compile-time type checking similar to strongly typed languages such as Java.

It was decided to work on both the frontend and backend in parallel to allow for rapid iteration and to avoid wasting time by
developing backend endpoints that would go unused by the frontend. The typical workflow was to implement an endpoint that
the frontend required, perform some basic testing to make sure it returned the correct data, then link it to the frontend which
would display it in a user-friendly way.

\section{Backend Implementation}

\subsection{Data Import}

After initial analysis of the dataset, a schema was designed to store the data in a database. The parts used by the
data importer are shown in figure~\ref{fig:importer_schema}.

A setup script is provided to convert the raw dataset into a format usable by the backend.
This script initializes the database, adds dummy data, parses the dataset, then imports everything
into the database. As this has to add embeddings which is slow, there is an option included
to only import a smaller subset of the data which allows for rapid iteration and testing of the import.

\subsection{Database}

A SQLite database is used to store the data, as it has a minimal footprint and is easy to set up. The connection
is exposed to database classes through dependency injection and interfaces, allowing for the database system to be
easily swapped out for another system such as PostgreSQL or MySQL.

The database is designed to be in third normal form, with repeated data such as ingredient names stored in a separate
table and linked to the recipe through foreign keys and junction tables. This allows for efficient querying and
maintains integrity through the use of these and further constraints, such as \texttt{UNIQUE} and \texttt{NOT NULL}.~\cite{codd_further_1972}

Using DI allows for a mock in-memory database to be used for testing, which gives each test case a clean slate to work with
that cannot be affected by other test cases. This is especially important for tests that modify the database, as they can
make assumptions about the initial state of the database.

\subsection{API}

The backend of the app is implemented as a REST API based on Express.js with SQLite
used for the database.

The API itself uses the Express.js framework, this was chosen as it is quick and easy to set up, is easily extensible
through external libraries, and provides high performance, in some cases even exceeding that of
compiled languages~\cite{karlsson_performance_2021}.

SQLite was chosen for the database as it is simple to set up, being implemented through
a library rather than requiring a dedicated server, while being somewhat portable with other
database systems.~\cite{kreibich_using_2010} This would allow for the database to be migrated to
another backend with a limited number of changes needing to be made.

One major difference between TypeScript and Java is that TypeScript uses structural typing, where objects are considered
the same if they have the same structure. While Java uses nominal typing and requires an interface to be explicitly
implemented by a class. This allows for different modules to be interoperable through sharing the same structure of objects.~\cite{gil_whiteoak_2008}
For this reason, along with the fact that the data transfer objects did not need their own logic, it was decided early on
to not use classes for objects retrieved from the database and instead to rely on interfaces only.

While many node packages do not include TypeScript types, most have declarations available through the \codelisting{@types} organization.
This allows packages such as \texttt{bcrypt} to be used with type safety and catches many errors at compile time rather than runtime as discussed
insection~\ref{sec:language}.

However, this is not a perfect solution as the quality of typing can vary greatly. For example, \texttt{better-sqlite3}
declares all queries as returning \texttt{unknown} which must be either asserted without type checking, or checked manually, neither of which
can be done at compile time. This is especially problematic in more complex queries and appears to be a limitation of TypeScript itself. This
problem and its potential solutions, are discussed in more detail in section~\ref{sec:search_complexity}

A subset of the needed endpoints were implemented initially to get experience in how the express framework works.
These consisted of basic getters and the first revision of the search endpoint, which at this point returned all
recipes with no filtering or ordering.

\subsection{Documentation}
The API was declared and documented using the OpenAPI format and converted
into TypeScript types using the \codelisting{openapi-typescript} package. These
generated types were then used to check the return the API's return types at compile time.

The \codelisting{express-openapi-validator} package was used separately to validate
request parameters against the OpenAPI schema. This implementation avoided having
to duplicate validation specifications and removed the possibility of having the values that
the server accepts differ from what was documented. There is additional functionality to validate
responses which is useful during development and the test suite, but can be disabled in production
for better performance.

One limitation that was not resolved with either of the above packages is that there is no validation
for whether the endpoints exist which lead to some confusion with endpoints returning 404 as the function to
register it with Express was not being called. A different combination of packages may have been able to check for this,
but was not considered due to time constraints.

Swagger UI was used to serve a generated HTML page that lists all endpoints declared in
the API specification, along with example values for both requests and responses and the ability to make requests in
the browser. This served as a quick way to make test requests with dummy data during development, and an easy-to-use
reference for the endpoints available to \codelisting{openapi-fetch}. See figure~\ref{fig:api_docs} for an example of the
generated documentation page.

\subsection{Machine Learning}
The similarity of recipes is evaluated using the Universal Sentence Encoder model discussed in Section~\ref{sec:recipe_similarity} to
embed and compare the names of recipes. Due to the high runtime cost of embedding sentences at approximately 100ms per string,
the embeddings are calculated during setup and stored in the database, organized by their source strings as shown in table~\ref{fig:embedding_table}.

\begin{table}[h]
    \centering
    \caption{\label{fig:embedding_table}An example of the contents of the embedding table.}
    \begin{tabular}{cc}
        \toprule
        \textbf{Original String} & \textbf{Embedding (format is an implementation detail)} \\\midrule
        Pasta & BLOB \\\bottomrule
    \end{tabular}
\end{table}

At runtime, the \codelisting{/api/v1/recipe/\{id\}/search} endpoint is used to filter and order recipes by their similarity score
relative to the query, and their availability and suitability for a user.
This took a significant amount of time to execute at first. Despite initial assumptions that this was due to the large amount of copying
created due to the implementation, was shown by profiling (see figure~\ref{fig:similarity_flamegraph}) to instead be in TensorFlow's
matrix maths functions.

This could be accelerated using a hardware-accelerated build of TensorFlow, which was not done due to the difficulty of installing
it on Windows. Given that machine learning performance was only a concern during setup, this was deemed a low-priority
issue and was not resolved.
\todo{Update this and add benchmark for after hardware accel and fix. Want to run the initial benchmark on my main PC}

\section{Frontend Implementation}

The frontend of the application was implemented in React using TypeScript, with Tailwind used for styling. These
frameworks allow for a quickly producing a responsive user interface that fetches data from the API with minimal
boilerplate code. Vite is used to hot-reload the application during development, allowing for rapid iteration, and
to build the application for deployment.

React was chosen as it is built around the concept of reusable components, which speeds up development and
improves code quality by avoiding duplicate code,~\cite{hordijk_harmfulness_2009} and abstracts away rendering
by only re-rendering components when their state changes. In addition, it has access to the vast majority of
Node packages along with many dedicated component libraries for common use cases such as routing between pages
which opened the possibility of re-using the same libraries for the front and back ends.~\cite{saks_javascript_2019}

\subsection{API Communication}
The \codelisting{openapi-fetch} package was used along with the types generated from the API documentation was
used to create a type-safe wrapper for the native \codelisting{fetch} function that is able to automatically
determine paths, parameters, and responses in a type-safe manner that would raise compiler errors if any breaking
changes were made. This would not have been possible with plain JavaScript or \texttt{fetch} as there would be no
information on what the API expected or returned.

\section{Common Components}

The backend and frontend are tightly integrated, and share many of the same components with the frontend acting
as little more than a wrapper around the backend. This reduced the time required to develop the app, would
make porting to a different frontend framework easier as there is minimal business logic in the frontend, and
allowed for basic testing of the backend before automated test cases were implemented.

\subsection{Recipe Search and Suggestions}
As the primary research question of the project, recipe search and similarity had the most work put into it.
These features initially used separate endpoints, but were later combined into a single endpoint due to the
high similarity of them, the only difference being the embedding used to compare the recipes (the search query
or the name of the recipe being compared).

\subsection{User Preferences}
User preferences are grouped into two types: dietary restrictions and ingredient preferences, each of which is stored
in its own table.

Dietary restrictions refer to categories of ingredients to avoid, such as \enquote*{meats} for vegetarians,
\enquote*{animal products} for vegans, and \enquote*{dairy} for those with lactose intolerance. These are implemented
as tags attached to ingredients and are manually assigned as part of the initial data script.

Ingredient preferences refer to specific ingredients that the user dislikes, such as \enquote*{mushrooms} or
\enquote*{chicken}. Both of these can be modified at any point through the account page.

During search, the preferences of all users the meal is being prepared for are combined to filter out recipes
that are disallowed by any of the users. See Section \ref{sec:variable_bind_count} for a discussion of how this was implemented.
Due to time constraints, the frontend only accounts for the logged-in user's preferences, but the endpoint
supports any number of users.

\subsection{Login and Account Creation}
As security was not a part of the research question, login and registration was kept simple with only a username
and password required. Password hashing was handled by the \codelisting{bcrypt} as this is a widely used and
accepted algorithm.~\cite{ntantogian_evaluation_2019} That is secure enough for the purposes of the app. However,
a commercial application would likely require more advanced security measures such as single-sign-on. Additionally,
there are no measures in place for account recovery or protection against malicious users.

The account creation subsystem was a late addition to the app, and as such, was not as well tested as other areas.

\section{Testing}

\subsection{Test Plan}
The backend of the app was unit tested using the Mocha.js framework
to verify that it functioned as intended and to catch any regressions. Tests for lower-level components
such as validation functions and the database were written before those for higher-level
components such as API endpoints. These test cases were executed on every push automatically,
with an alert being sent if any of them failed.

\subsubsection{Testing Methodologies}

There are two main types of analysis that can be used to test software: static and dynamic. In these groups, there are
four levels of testing: unit, integration, system, and acceptance.~\cite{luo_software_2001}

TypeScript inherently provides some level of static analysis, which can catch common categories of errors such as
incorrect types, missing properties, and null values. As mentioned in section~\ref{sec:language}, this is able to
catch many issues pre-emptively at compile time. This could be considered a combination of unit, and to a lesser extent,
integration testing.

Due to the analytical nature of the project, user testing was not considered necessary. Instead, the focus was on ensuring
that the system was reliable, and the results were as expected. This was done using automated unit and integration tests for
objective areas, i.e., \enquote*{did it work?}, and manual testing for subjective areas such as the quality of the recipe
suggestions which acted as a form of system testing. If it was performed, user testing would have acted as acceptance testing,
as defined by~\cite{luo_software_2001}.

See Table~\ref{tab:test_types} for a summary of the testing levels and how they were implemented.

\begin{table}[hp]
    \centering
    \caption{\label{tab:test_types}The four levels of testing and how they were implemented.}
    \begin{tabular}{cll}\toprule
        \textbf{Type} & \textbf{Static Analysis} & \textbf{Dynamic Analysis} \\\midrule
        \textbf{Unit} & Type Checking & Mocha.js \\
        \textbf{Integration} & (limited) Type Checking & Supertest \\
        \textbf{System} & None & Manual Testing \\
        \textbf{Acceptance} & None & None \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Unit and Integration Testing}
The backend of the application was tested using the Mocha testing framework, with Supertest
used to simulate API calls. Finally, coverage reports were generated with the Istanbul framework.

The test cases could be run manually using the commands \texttt{npm run test} and \texttt{npm run test:coverage}
and were run automatically after every push via a GitHub action.

Each run of the test suite generated a test report, shown in figure~\ref{fig:test_report}. This report
documents the number of tests run (shown as a \enquote*{\texttt{.}} for each test in the reporter used),
and, if any tests failed, the error message, stack trace, and the expected output compared to the actual output.

Coverage reports were likewise generated when the suite is run in coverage mode, shown in figure~\ref{fig:coverage_report}.
This report is in HTML format and shows the percentage of lines, functions, and branches that were executed at least once
during the test run on a per-file, per-directory, and overall basis. This allowed for easy identification of areas that
were lacking test cases and could be improved. It did not, however, reflect the quality of the tests themselves.~\cite{meyer_is_2012}
For this reason, there was no target coverage and the focus was on regression testing.

When a test failed during an automated run, an email was sent, as shown in figure~\ref{fig:test_failure}, to alert
about the issue. This email linked to the test logs which detailed the error in the same format as if the test had been run
locally.

Test cases were, for the most part, written after the functions they were intended to test,
as there is limited evidence for test-driven development leading to higher
quality tests or greater code coverage.~\cite{tosun_effectiveness_2018,madeyski_impact_2010}

By writing unit tests immediately after implementing a feature, and automating the entire suite to run
on every push, the \enquote*{test early, test often} principle was followed. This allowed for
issues to be caught early and narrowed down to the specific function that caused them.~\cite{olan_unit_2003}

\begin{figure}[p]
    \caption{\label{fig:test_case}An example of a test case from \texttt{testGetEmbedding.ts}}
    \begin{minted}{ts}
// Name of the test suite. By convention, this is in the
// function or class being tested
describe('ml/getEmbedding', function () {
  // Name of the test case. By convention, this reads as
  // <function> should <expected result>
  it('should return similar embeddings for similar strings',
    async function () {
      // Get the embeddings for two strings.
      // These should be similar as the strings are similar
      // We need to use await as the function is asynchronous
      const emb1 = await getEmbedding('Hello world')
      const emb2 = await getEmbedding('Hello there')
      assert(
        // Assert that this condition is true
        // In this case, that similarity is greater than a threshold
        getSimilarity(emb1.embedding, emb2.embedding) > 0.8,
        // If it is not, this message will be displayed
        'similarity should be high'
      )
    }
  )

  // Another test case. Very similar to the first, just
  // testing the opposite condition
  it('should return dissimilar embeddings for dissimilar strings',
    async function () {
      const emb1 = await getEmbedding('Hello world')
      const emb2 = await getEmbedding(
        'Did you ever hear the tragedy of Darth Plagueis the Wise?'
      )
      assert(
        getSimilarity(emb1.embedding, emb2.embedding) < 0.2,
        'similarity should be low'
      )
    }
  )
})
    \end{minted}
\end{figure}

\subsubsection{Regression Testing}

Whenever an issue was discovered in the backend, the first step taken was to
introduce new unit tests based on the expected behaviour. The faulty code was
then fixed, and the tests kept in place. This helps to prevent the issue being
reintroduced at a later date.

One example of regression testing is that, after discovering that embeddings were
not being properly decoded from the database (see Section~\ref{sec:embedding_round_trip}),
test cases were implemented to make sure that an identical array would be returned
after encoding to a buffer back again in \texttt{testBufferFloat32Array.ts}

\subsection{Code Quality Evaluation}
The MegaLinter tool~\cite{vuillamy_megalinter_nodate} was configured to run on every pull request on the application's Git repository. This includes a wide array
of checks to ensure that style guides were adhered to and to catch potential issues, such as making potentially incorrect
assumptions about an object's type. This generates a report that is attached to each pull request detailing
any issues that were found (figure~\ref{fig:metalinter_report}).

As with the unit tests action, an email is sent if any of the checks fail.

The configuration for MegaLinter and its linters can be found in the root directory of the repository.
Certain issues, such as incorrect indentation, are fixed automatically when a pull request is created, while others
must be corrected manually. Overall, MegaLinter aids in ensuring code quality and preventing the use of bad practices.

As an example, the jscpd linter included with MegaLinter checks for duplicate code blocks. These are indications of code
smell that should be extracted into their own functions and reused.~\cite{fowler_refactoring_1997}

Another design flaw that can lead to code smell is the existence of \enquote*{god} objects
and methods that include large amounts of logic, increasing complexity~\cite{marinescu_measurement_2005,vaucher_tracking_2009}
Care was taken to avoid the introduction of these into the \chef{} app, although no specific
metrics were used. One notable exception discussed is the search query and endpoint discussed in
Section~\ref{sec:search_complexity}.

\section{Investigation}
\todoinline{
    You should also discuss the investigation process. This does not mean that you have to repeat what you have already said about the design of your investigation,
    but you should comment on what happened during the investigation, e.g.\ how you conducted the experiments, how you collected data, or anything new or interesting
    that occurred, and perhaps add details that arose from events. You may have had to adapt your approach in some way. If things went wrong, or in the event
    you took an approach different from the one you planned, then you can explain what happened, what you did about it, and why. (It isn't expected that everything will
    go according to the design: how you deal with situations that arise can be a very interesting part of your report.) If you have produced deliverables, you can present
    and discuss these. Anything that would impede the flow of your chapter can be provided in the appendices or, if large, on your disk.
}

\section{Results}

\todo{This should be in the methodology section as it discusses preprocessing}
The dataset used contains a number of meaningless and nonsensical recipes (see
figure~\ref{fig:bad_recipe_entry}), which were imported by the \chef{} system during setup.
A possible reason for these being included in RecipeNLG is that some
data sources scraped for it allow for user-submitted recipes, this contributes to
an unclean and noisy dataset.~\cite{kicherer_what_2018}

\begin{figure}[p]
    \centering
    \caption{\label{fig:bad_recipe_entry}A nonsensical recipe that was included in the dataset.}
    \includegraphics[scale=0.75]{figures/bad_recipe_entry.png}
\end{figure}

A possible solution for this issue would have been to further pre-process the data beyond what is
discussed in Section~\ref{sec:data_pre_process} to remove or correct noisy entries that would otherwise
harm the quality of the system.~\cite{garcia_big_2016} This was not considered a priority due to time
constraints and the fact that the noise had minimal impact on the system's results.

\subsection{Similar Recipes}

The recipe similarity algorithm was found to be fairly effective at suggesting related items.
For example, \enquote*{Spaghetti Casserole} is evaluated as having 90\% similarity to
\enquote*{Spinach Casserole} and has high similarity to other casseroles. A more surprising
outcome is the suggestion of seemingly unrelated recipes, such as how \enquote*{Potato Salad}
was rated as more similar to the casserole than casserole is to other Italian recipes such as
pasta.

As recipe similarity only takes the name into account, it is unable to find similarities
in other areas such as ingredient list or directions. An alternative approach to using
recipe names could have been to take the ingredients and directions of the recipe into account,
similar to the strategy used by Freyne and Berkovsky.~\cite{freyne_intelligent_2010}

Despite these limitations, the similar recipe suggestion algorithm appears to be a fairly effective
means of finding new recipes. Most of the suggested recipes appear to be closely related to the entry they are
compared with. However, some recipes are suggested despite seemingly having little or not correlation with the
recipe they are contained to as shown in table~\ref{tab:similar_spaghetti} where \enquote*{Potato Salad} is considered
to be similar to \enquote*{Spaghetti}. With other recipes, the suggestions have little variety as shown by
table~\ref{tab:similar_apple_crumble}. This issue is part of the reason that
\hyperref[req:too_similar]{FR\arabic{toosimilarid}} was not considered to be implemented.

\begin{table}[p]
    \centering
    \caption{\label{tab:similar}Similar recipes for \enquote*{Spaghetti} and \enquote*{Apple Crumble}. Most results are omitted for brevity.}
    \subfloat[Spaghetti]{\label{tab:similar_spaghetti}
        \begin{tabular}{cc}\toprule
            \textbf{Recipe} & \textbf{Similarity} \\\midrule
            Spaghetti Casserole & 85\% \\
            Pasta & 76\% \\
            Potato Salad & 74\% \\
            Noodles & 71\% \\
            Ravioli & 71\% \\
            Garlic Cheese & 71\% \\
            Garlic Bread & 69\% \\
            Pizza & 69\% \\
            \bottomrule
        \end{tabular}}
    \quad
    \subfloat[Apple Crumble]{\label{tab:similar_apple_crumble}
        \begin{tabular}{cc}\toprule
            \textbf{Recipe} & \textbf{Similarity} \\\midrule
            Apple Custard Pie & 81 \% \\
            Apple Nut Cake & 77\% \\
            Apple Muffins & 77\% \\
            Apple Pie & 72\% \\
            Apple Custard Pie & 72\% \\
            Raspberry Pie & 72\% \\
            \bottomrule
        \end{tabular}}
\end{table}

\subsubsection{Recipe Search}
\todo{Recipe search, based on same model as similar}

Searching of recipes is implemented nearly identically to the similar recipe comparison, only differing in that
it compares the user's query rather than the selected recipe's name.

An area that was not accounted for was that Universal Sentence Encoder is case-sensitive.~\todo{source}
Because of this, searching for \enquote*{pasta} would yield different results to searching \enquote*{Pasta} as
shown in figure~\ref{fig:search_case_sensitive}

\begin{figure}[p]
    \centering
    \caption{\label{fig:search_case_sensitive}An example of the case sensitivity of searches.}
    \includegraphics[width=0.45\textwidth]{figures/search_lower.png}
    \includegraphics[width=0.45\textwidth]{figures/search_upper.png}
\end{figure}

\subsection{Meal Type Prediction}
\todo{Meal type prediction}

\todoinline{
    The final part of is the presentation of your results. You may have quantitative or qualitative data, or even both. The best way of presenting your results will be determined
    by the type of data and the nature of your investigation but will usually involve summarising the data in some way.  Data analysis is too large and varied a topic to discuss
    in detail here, and how you do it will be very dependent on the project that you are doing. You are likely to have found out about appropriate methods as part of your
    project. In some cases, it may be appropriate to present calculations, e.g.\ to demonstrate how performance figures are derived. If you are doing statistical analyses,
    it will be helpful include levels of significance with the results where applicable. The presentation of qualitative data may involve summarising it, identifying significant
    factors, including representative examples, discussing interesting cases or critical incidents in depth, the use of quotations and illustrations, or identifying significant
    categories of content from textual data and looking at how often they occur.  For example, if you had been interviewing people about their use of information, you might
    identify categories related to the type of information, the method of access, etc.
    It may be helpful to use diagrams, charts or tables to present the work or the results. These should come with enough explanation for the reader to make sense of what
    you are showing.

    In general, raw data in the body of your report will be limited to small elements or examples that that you are discussing. Further data can go in your appendices or
    (if bulky) on your OneDrive folder, and you should tell the reader where to look for it. Identifiable personal data should never be included in any part of your submission.
    If you need to refer to individuals, you can mention \enquote*{Participant A} in your study, or in organisational settings it may be appropriate to refer to someone by their job title.
}

\todo{Results}
