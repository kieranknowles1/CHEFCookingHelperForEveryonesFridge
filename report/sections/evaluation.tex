\todoinline{
    Evaluation
    You should present two or three critical evaluations of your work, in separate sections or chapters.

    •	Discussion and evaluation of findings
    If you have conducted an investigation into a research question or hypothesis, this is where you discuss the meaning of your results.
    What answers have you found to the question that you are investigating? You should explicitly relate your findings to the problem, question or hypothesis,
    and discuss how far you have answered that question or solved the problem, whether your results support or refute your hypotheses, etc. You may wish to
    compare your findings with those of other work that you have discussed in your literature review. The marking scheme asks you do discuss your confidence in
    your findings and how far they can be generalised. Are there factors that affect the reliability of your results or conclusions? If this is relevant to your project,
    are your results statistically significant? Would you expect similar studies to achieve the same results? Would you expect that people carrying out similar work in
    a different organisation would come to similar conclusions? Remember that it is often not possible to generalise from a single case,
    or from a small number of tests participants etc.

    •	An evaluation of your product
    If you have built a product, you should evaluate your product from a technical point of view. You need to identify the strengths and weaknesses of your product
    in meeting its requirements, and review the possible alternative technical approaches to its design and implementation. Beware of the 'anecdotal' evaluation - you
    are expected to take a critical view and justify your argument.
    You should try to give evidence to support your evaluation: this could include the result of testing and user trials, feedback from clients, etc. Do not be afraid
    to discuss weaknesses: your evaluation will be assessed by its validity, regardless of the quality of the product. If your product is not software, you will need
    to be particularly careful in planning how it will be evaluated. Be sure that enough time is allowed for gathering necessary evidence: it is essential that this
    is thought about early in the project.
    •	An evaluation of the project process
    Every project report should have a session/chapter for the evaluation of the project process. This section is fully described in your marking scheme. The emphasis
    should be on the learning process and on how well you managed your project work. What have you learned, and what would you do differently in future? Achievement
    of relevant objectives should be assessed, so look at the objectives in your Terms of Reference and see which ones are relevant here and which are part of the
    product/findings evaluation. You can reflect on your project plan and suggest other plans that might have worked better. You may also be able to discuss legal,
    social, ethical, or professional issues that have arisen and comment on your handling of them.
}

\section{Project Process}
\todo{Project Process}

\subsection{Issues Encountered}
\todo{Make this a table}

An unexpected issue during data import was the number of unique ingredients found which totalled ~200,000. This was worked
around by instead only including a more limited list of ingredients that would suffice to import a target of \todo{Target} of
the total dataset. This led to \todo{How many?} recipes being available for use.

The \codelisting{fraction.js} package used to parse ingredient amounts could not be imported at first with an error message
stating that \enquote*{The current file is a CommonJS module ... however, the referenced file is an ECMAScript module}.
This issue had previously been reported by another user (\href{https://github.com/rawify/Fraction.js/issues/70}{https://github.com/rawify/Fraction.js/issues/70})
and was worked around by downgrading to version 4.3.4 of \codelisting{fraction.js} while the latest version at the time
of writing was 4.3.7.

The \codelisting{better-sqlite3} package declares row IDs as being \codelisting{number | bigint}
while \codelisting{openapi-typescript} generates \codelisting{number}, even if the type in the API
declaration lists the type as \codelisting{integer}. This initially required additional casting in every endpoint
that returns an ID. As a workaround, row IDs in the application were instead declared as \codelisting{number}
and an error is thrown if an insert produces a \codelisting{bigint} ID. As the maximum integer that can be safely stored as a number is
$(2^{53} - 1)$ \todo{source}, the chances of this happening were considered nonexistent.

When passing embeddings through SQLite queries, they are converted to SQLite's BLOB type which is then passed to TypeScript as a \codelisting{Buffer}
before being decoded as a \codelisting{Float32Array}.
The initial implementation of the conversion had an issue where the buffer would be parsed as an array of 2048 8-bit integers,
rather than 512 32-bit floats. This issue greatly reduced both the effectiveness and performance of the similarity comparisons.
The issue was fixed by correctly parsing the buffer and by adding a check to the \codelisting{getSimilarity} function to assert
that both of the arrays passed had the expected 512 elements.

SQLite does not support passing arrays to the bind parameters of queries, which was
necessary when implementing user preferences in the search query. This issue
was resolved by generating a temporary table containing the preferences
that only exists for the duration of the query. See figure \ref{fig:temp_table_substitution}
for an example of where this is used.

The signup endpoint was initially coded to return \texttt{200 OK} when it should have returned
\texttt{201 Created}. This issue was caught while generating test cases with GitHub Copilot and resolved
soon after.

\begin{figure}[h]
    \caption{\label{fig:temp_table_substitution}A temporary table that is substituted into a query with \$\{expr\}}
    \raggedright

    \codelisting{
        (SUM(CASE WHEN ingredient\textunderscore{}tag.tag\textunderscore{}id IN
        (SELECT tag\textunderscore{}id FROM \$\{bannedTagsTable\}) THEN 1 ELSE 0 END) = 0)
    }
\end{figure}
