\section{Project Process}

\subsection{Issues Encountered}

\subsubsection{Number of Ingredients}
An unexpected issue during data import was the number of unique ingredients found which totalled ~200,000, many
of which were duplicates or misspellings of the same ingredient. This was worked around by instead only including a
more limited list of ingredients that would suffice to import a target of 1\% of the total dataset. This led to 30,000 recipes
being available for use. An alternative would have been to use a different dataset that included fewer ingredients or the same
set in a cleaner format, or to have used a dedicated library for ingredient parsing.

\subsubsection{Fraction.js}
The \codelisting{fraction.js} package used to parse ingredient amounts could not be imported at first with an error message
stating that \enquote*{The current file is a CommonJS module ... however, the referenced file is an ECMAScript module}.
This issue had previously been reported by another user (\href{https://github.com/rawify/Fraction.js/issues/70}{https://github.com/rawify/Fraction.js/issues/70})
and was worked around by downgrading to version 4.3.4 of \codelisting{fraction.js} while the latest version at the time
of writing was 4.3.7.

\subsubsection{Row ID Type}
The \codelisting{better-sqlite3} package declares row IDs as being \codelisting{number | bigint}
while \codelisting{openapi-typescript} generates \codelisting{number}, even if the type in the API
declaration lists the type as \codelisting{integer}. This initially required additional casting in every endpoint
that returns an ID. As a workaround, row IDs in the application were instead declared as \codelisting{number}
with checks to throw an exception if an insert produces a \codelisting{bigint} ID. As the maximum integer that can be safely stored as a number is
$(2^{53} - 1)$ according to Mozilla's documentation, and SQLite allocates IDs using \texttt{MAX(id) + 1},
the chances of this happening were considered nonexistent.

\subsubsection{Embedding Round Trip Accuracy}\label{sec:embedding_round_trip}
When passing embeddings through SQLite queries, they are converted to SQLite's BLOB type which is then passed to TypeScript as a \codelisting{Buffer}
before being decoded as a \codelisting{Float32Array}.
The initial implementation of the conversion had an issue where the buffer would be parsed as an array of 2048 8-bit integers,
rather than 512 32-bit floats. This issue greatly reduced both the effectiveness and performance of the similarity comparisons.
The issue was fixed by correctly parsing the buffer, adding a check to the \codelisting{getSimilarity} function to assert
that both of the arrays passed had the expected 512 elements, and adding a test case to
\texttt{testBufferFloat32Array} (see Figure \ref{fig:buffer_float_array})

\subsubsection{Variable Bind Parameter Count}\label{sec:variable_bind_count}
SQLite does not support passing arrays to the bind parameters of queries, which was
necessary when implementing user preferences in the search query. This issue
was resolved by generating a temporary table containing the preferences
that only exists for the duration of the query. See figure \ref{fig:temp_table_substitution}
for an example of where this is used.

\begin{figure}[h]
    \caption{\label{fig:temp_table_substitution}A temporary table that is substituted into a query with \$\{expr\}}
    \raggedright

    \codelisting{
        (SUM(CASE WHEN ingredient\textunderscore{}tag.tag\textunderscore{}id IN
        (SELECT tag\textunderscore{}id FROM \$\{bannedTagsTable\}) THEN 1 ELSE 0 END) = 0)
    }
\end{figure}

\subsubsection{Incorrect Response Code}
The sign-up endpoint was initially coded to return \texttt{200 OK} when it should have returned
\texttt{201 Created} in accordance with RESTful response code principles. This issue was caught
while generating test cases with GitHub Copilot and resolved soon after.

\subsection{Potential Improvements}

\subsubsection{Choice Overload}\label{sec:choice_overload}
As discussed previously in Section \ref{sec:overload_intro},~\cite{bollen_understanding_2010} found that
provinding fewer choices can, counterintuitively, lead to higher satisfaction. Instead of giving
many options for the user to choose from. In Bollen et al.'s study, the Lin-20 condition, in which
the 5 highest-rated items were shown followed by every 100th item, had the highest perceived variety
and satisfaction without an adverse effect on choice difficulty.

Another option that could be combined with this would have been to display personalized and categorized
recommendations similar to those found on Netflix. This would be very similar to Lin-20 when suggesting
similar recipes, but would show categorized recommendations on the find recipes page based on attributes
such as the cuisine evaluated against the user's preferences.~\cite{gomez-uribe_netflix_2016}

\subsubsection{Query Complexity and Checking}\label{sec:search_complexity}
One potential area for improvement would have been to use an ORM instead of
writing queries directly. This would improve maintainability of the more complex
queries, such as the one used for searching recipes as shown in figure \ref{fig:search_query}.
Including queries of such complexity has been shown as having a negative impact on the perceived
maintainability of a codebase~\cite{yamashita_code_2012} (n=6).

The endpoint and query function included many parameters by the end of development
(see figure \ref{fig:search_params}), all of which were needed to provide the functionality
required by the frontend, but made maintaining, extending, or testing the code more difficult.
An ORM could have abstracted away much of this complexity and allowed for compile-time checking
with less boilerplate code or needs for test cases.
