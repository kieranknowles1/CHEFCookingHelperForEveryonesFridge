\todoinline{
    Evaluation
    You should present two or three critical evaluations of your work, in separate sections or chapters.

    •	Discussion and evaluation of findings
    If you have conducted an investigation into a research question or hypothesis, this is where you discuss the meaning of your results.
    What answers have you found to the question that you are investigating? You should explicitly relate your findings to the problem, question or hypothesis,
    and discuss how far you have answered that question or solved the problem, whether your results support or refute your hypotheses, etc. You may wish to
    compare your findings with those of other work that you have discussed in your literature review. The marking scheme asks you do discuss your confidence in
    your findings and how far they can be generalised. Are there factors that affect the reliability of your results or conclusions? If this is relevant to your project,
    are your results statistically significant? Would you expect similar studies to achieve the same results? Would you expect that people carrying out similar work in
    a different organisation would come to similar conclusions? Remember that it is often not possible to generalise from a single case,
    or from a small number of tests participants etc.

    •	An evaluation of your product
    If you have built a product, you should evaluate your product from a technical point of view. You need to identify the strengths and weaknesses of your product
    in meeting its requirements, and review the possible alternative technical approaches to its design and implementation. Beware of the 'anecdotal' evaluation - you
    are expected to take a critical view and justify your argument.
    You should try to give evidence to support your evaluation: this could include the result of testing and user trials, feedback from clients, etc. Do not be afraid
    to discuss weaknesses: your evaluation will be assessed by its validity, regardless of the quality of the product. If your product is not software, you will need
    to be particularly careful in planning how it will be evaluated. Be sure that enough time is allowed for gathering necessary evidence: it is essential that this
    is thought about early in the project.
    •	An evaluation of the project process
    Every project report should have a session/chapter for the evaluation of the project process. This section is fully described in your marking scheme. The emphasis
    should be on the learning process and on how well you managed your project work. What have you learned, and what would you do differently in future? Achievement
    of relevant objectives should be assessed, so look at the objectives in your Terms of Reference and see which ones are relevant here and which are part of the
    product/findings evaluation. You can reflect on your project plan and suggest other plans that might have worked better. You may also be able to discuss legal,
    social, ethical, or professional issues that have arisen and comment on your handling of them.
}

\section{Project Process}
\todo{Project Process}

\subsection{Issues Encountered}

\subsubsection{Number of Ingredients}
An unexpected issue during data import was the number of unique ingredients found which totalled ~200,000. This was worked
around by instead only including a more limited list of ingredients that would suffice to import a target of 1\% of
the total dataset. This led to 30,000 recipes being available for use.

\subsubsection{Fraction.js}
The \codelisting{fraction.js} package used to parse ingredient amounts could not be imported at first with an error message
stating that \enquote*{The current file is a CommonJS module ... however, the referenced file is an ECMAScript module}.
This issue had previously been reported by another user (\href{https://github.com/rawify/Fraction.js/issues/70}{https://github.com/rawify/Fraction.js/issues/70})
and was worked around by downgrading to version 4.3.4 of \codelisting{fraction.js} while the latest version at the time
of writing was 4.3.7.

\subsubsection{Row ID Type}
The \codelisting{better-sqlite3} package declares row IDs as being \codelisting{number | bigint}
while \codelisting{openapi-typescript} generates \codelisting{number}, even if the type in the API
declaration lists the type as \codelisting{integer}. This initially required additional casting in every endpoint
that returns an ID. As a workaround, row IDs in the application were instead declared as \codelisting{number}
with checks to throw an exception if an insert produces a \codelisting{bigint} ID. As the maximum integer that can be safely stored as a number is
$(2^{53} - 1)$~\cite{noauthor_numbermax_safe_integer_2023}, and SQLite allocates IDs using \texttt{MAX(id) + 1},
the chances of this happening were considered nonexistent.

\subsubsection{Embedding Round Trip Accuracy}\label{sec:embedding_round_trip}
When passing embeddings through SQLite queries, they are converted to SQLite's BLOB type which is then passed to TypeScript as a \codelisting{Buffer}
before being decoded as a \codelisting{Float32Array}.
The initial implementation of the conversion had an issue where the buffer would be parsed as an array of 2048 8-bit integers,
rather than 512 32-bit floats. This issue greatly reduced both the effectiveness and performance of the similarity comparisons.
The issue was fixed by correctly parsing the buffer, adding a check to the \codelisting{getSimilarity} function to assert
that both of the arrays passed had the expected 512 elements, and adding a test case to
\texttt{testBufferFloat32Array} (see Figure \ref{fig:buffer_float_array})

\subsubsection{Variable Bind Parameter Count}\label{sec:variable_bind_count}
SQLite does not support passing arrays to the bind parameters of queries, which was
necessary when implementing user preferences in the search query. This issue
was resolved by generating a temporary table containing the preferences
that only exists for the duration of the query. See figure \ref{fig:temp_table_substitution}
for an example of where this is used.

\begin{figure}[h]
    \caption{\label{fig:temp_table_substitution}A temporary table that is substituted into a query with \$\{expr\}}
    \raggedright

    \codelisting{
        (SUM(CASE WHEN ingredient\textunderscore{}tag.tag\textunderscore{}id IN
        (SELECT tag\textunderscore{}id FROM \$\{bannedTagsTable\}) THEN 1 ELSE 0 END) = 0)
    }
\end{figure}

\subsubsection{Incorrect Response Code}
The signup endpoint was initially coded to return \texttt{200 OK} when it should have returned
\texttt{201 Created}. This issue was caught while generating test cases with GitHub Copilot and resolved
soon after.

\subsection{Potential Improvements}

\subsubsection{Query Complexity and Checking}\label{sec:search_complexity}
One potential area for improvement would have been to use an ORM instead of
writing queries directly. This would improve maintainability of the more complex
queries, such as the one used for searching recipes as shown in figure \ref{fig:search_query}.
Including queries of such complexity has been shown as having a negative impact on the perceived
maintainability of a codebase~\cite{yamashita_code_2012} (n=6).

The endpoint and query function included many parameters by the end of development
(see figure \ref{fig:search_params}), all of which were needed to provide the functionality
required by the frontend, but made maintaining, extending, or testing the code more difficult.
An ORM could have abstracted away much of this complexity and allowed for compile-time checking
with less boilerplate code or needs for test cases.
